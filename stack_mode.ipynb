{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "stack mode",
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'apple-disease-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F2068739%2F3433042%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240520%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240520T053237Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D2130868ada93cc9abfe0e665e99863a98a46fc678b2b67135ff484eeae143bbea9c5bb0b85a03a4e79877535e24411498e6909550c83f9e466fcd2e156cffb6317080f4b26c3fc6b6a200fbe952c42babfe1d6861d35f88db7913184d04741efbdfcb4e1c615683265a84348a8d5e00c075dae13e85e7cb276b1dfaffef54d0b241caee808b191a83e6a6105b8170b0f0d92e2d26e31e83e151f0d8ba096f450903108fe7724b2720f7edea3f890650a4f7a13be8fde6445f52465d4ba55504642aaea0b54987f3fbfb7577a68f62bc3ed53f26138a3946f1afb62f7a44db79eb328f6050ca3b4fb5274083e047d24222117c4f03cd47822c89154c04fe9ad25,d-kap:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F2464177%2F4176149%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240520%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240520T053237Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D39c4c91aac7f38789eea67d33f7d132d077a421e5d78b8f1c0618f70e80d446150b2f70190cfad3843fd399a519b96e985b8de92cfb547e9773772305a0e3ce83d9ec8e80f3c5979f7106566f11b79f46a77b5289193ffd1ae71d2e7878976e0c66501cf8fc35e3c5badb72070b011a6909236b2ad77454c5174bd86e54546419598c63950f77bbe117d346c965cb8908abb6c047eb4d31fa1b870c0422e7e42309186caf215fbb596572d2d61d268ef6eee805350446eb4eee5b63c670171e1ef3f803253593b7c4688b2bb7dcda368644fef32d50481e824e4371c8dcd19263e13fb834f85b1e336e467263977ae6f882f2f886b0cee17625e11705bc51984'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "Wf_ECpaMfQDE"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2023-05-04T07:33:26.272858Z",
          "iopub.execute_input": "2023-05-04T07:33:26.273347Z",
          "iopub.status.idle": "2023-05-04T07:33:41.665929Z",
          "shell.execute_reply.started": "2023-05-04T07:33:26.273309Z",
          "shell.execute_reply": "2023-05-04T07:33:41.664861Z"
        },
        "trusted": true,
        "id": "DNDw2YG9fQDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Set the source directory\n",
        "src_dir = \"/kaggle/input/apple-disease-dataset/datasets/train\"\n",
        "\n",
        "# Set the destination directory\n",
        "dst_dir = \"/kaggle/working/apple-disease-dataset/datasets/train\"\n",
        "\n",
        "# Remove the destination directory if it already exists\n",
        "if os.path.exists(dst_dir):\n",
        "    shutil.rmtree(dst_dir)\n",
        "\n",
        "# Copy the contents of the source directory to the destination directory\n",
        "shutil.copytree(src_dir, dst_dir)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-04T07:33:54.406674Z",
          "iopub.execute_input": "2023-05-04T07:33:54.407334Z",
          "iopub.status.idle": "2023-05-04T07:35:04.291385Z",
          "shell.execute_reply.started": "2023-05-04T07:33:54.407297Z",
          "shell.execute_reply": "2023-05-04T07:35:04.290188Z"
        },
        "trusted": true,
        "id": "xEkHKdy4fQDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Set the source directory\n",
        "src_dir = \"/kaggle/input/apple-disease-dataset/datasets/test\"\n",
        "\n",
        "# Set the destination directory\n",
        "dst_dir = \"/kaggle/working/apple-disease-dataset/datasets/test\"\n",
        "\n",
        "# Remove the destination directory if it already exists\n",
        "if os.path.exists(dst_dir):\n",
        "    shutil.rmtree(dst_dir)\n",
        "\n",
        "# Copy the contents of the source directory to the destination directory\n",
        "shutil.copytree(src_dir, dst_dir)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-04T07:35:04.293479Z",
          "iopub.execute_input": "2023-05-04T07:35:04.293782Z",
          "iopub.status.idle": "2023-05-04T07:35:21.914514Z",
          "shell.execute_reply.started": "2023-05-04T07:35:04.293754Z",
          "shell.execute_reply": "2023-05-04T07:35:21.913525Z"
        },
        "trusted": true,
        "id": "HPgQJQUSfQDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "# Define paths to source directory and two destination directories\n",
        "src_dir = r'/kaggle/input/d-kap/APPLE_DISEASE_DATASET/APPLE ROT LEAVES'\n",
        "dst_dir_1 = '/kaggle/working/apple-disease-dataset/datasets/train/black_rot'\n",
        "dst_dir_2 = '/kaggle/working/apple-disease-dataset/datasets/test/black_rot'\n",
        "\n",
        "# Create the destination directories if they don't already exist\n",
        "if not os.path.exists(dst_dir_1):\n",
        "    os.makedirs(dst_dir_1)\n",
        "if not os.path.exists(dst_dir_2):\n",
        "    os.makedirs(dst_dir_2)\n",
        "\n",
        "# Get a list of all files in the source directory\n",
        "files = os.listdir(src_dir)\n",
        "\n",
        "# Calculate the number of files to copy to each destination directory (60% and 40%)\n",
        "num_files_to_copy_1 = int(len(files) * 0.6)\n",
        "num_files_to_copy_2 = len(files) - num_files_to_copy_1\n",
        "\n",
        "# Randomly select the files to copy to each destination directory\n",
        "files_to_copy_1 = random.sample(files, num_files_to_copy_1)\n",
        "files_to_copy_2 = list(set(files) - set(files_to_copy_1))\n",
        "\n",
        "# Copy the selected files to each destination directory\n",
        "for file_name in files_to_copy_1:\n",
        "    src_file_path = os.path.join(src_dir, file_name)\n",
        "    dst_file_path = os.path.join(dst_dir_1, file_name)\n",
        "    shutil.copy(src_file_path, dst_file_path)\n",
        "\n",
        "for file_name in files_to_copy_2:\n",
        "    src_file_path = os.path.join(src_dir, file_name)\n",
        "    dst_file_path = os.path.join(dst_dir_2, file_name)\n",
        "    shutil.copy(src_file_path, dst_file_path)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-04T07:35:21.916049Z",
          "iopub.execute_input": "2023-05-04T07:35:21.916436Z",
          "iopub.status.idle": "2023-05-04T07:35:22.890386Z",
          "shell.execute_reply.started": "2023-05-04T07:35:21.916399Z",
          "shell.execute_reply": "2023-05-04T07:35:22.889044Z"
        },
        "trusted": true,
        "id": "QFKllLcCfQDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "# Define paths to source directory and two destination directories\n",
        "src_dir = r'/kaggle/input/d-kap/APPLE_DISEASE_DATASET/HEALTHY LEAVES'\n",
        "dst_dir_1 = '/kaggle/working/apple-disease-dataset/datasets/train/healthy'\n",
        "dst_dir_2 = '/kaggle/working/apple-disease-dataset/datasets/test/healthy'\n",
        "\n",
        "# Create the destination directories if they don't already exist\n",
        "if not os.path.exists(dst_dir_1):\n",
        "    os.makedirs(dst_dir_1)\n",
        "if not os.path.exists(dst_dir_2):\n",
        "    os.makedirs(dst_dir_2)\n",
        "\n",
        "# Get a list of all files in the source directory\n",
        "files = os.listdir(src_dir)\n",
        "\n",
        "# Calculate the number of files to copy to each destination directory (60% and 40%)\n",
        "num_files_to_copy_1 = int(len(files) * 0.6)\n",
        "num_files_to_copy_2 = len(files) - num_files_to_copy_1\n",
        "\n",
        "# Randomly select the files to copy to each destination directory\n",
        "files_to_copy_1 = random.sample(files, num_files_to_copy_1)\n",
        "files_to_copy_2 = list(set(files) - set(files_to_copy_1))\n",
        "\n",
        "# Copy the selected files to each destination directory\n",
        "for file_name in files_to_copy_1:\n",
        "    src_file_path = os.path.join(src_dir, file_name)\n",
        "    dst_file_path = os.path.join(dst_dir_1, file_name)\n",
        "    shutil.copy(src_file_path, dst_file_path)\n",
        "\n",
        "for file_name in files_to_copy_2:\n",
        "    src_file_path = os.path.join(src_dir, file_name)\n",
        "    dst_file_path = os.path.join(dst_dir_2, file_name)\n",
        "    shutil.copy(src_file_path, dst_file_path)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-04T07:35:22.893013Z",
          "iopub.execute_input": "2023-05-04T07:35:22.893318Z",
          "iopub.status.idle": "2023-05-04T07:35:23.359795Z",
          "shell.execute_reply.started": "2023-05-04T07:35:22.89329Z",
          "shell.execute_reply": "2023-05-04T07:35:23.358739Z"
        },
        "trusted": true,
        "id": "aAcK44yZfQDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "# Define paths to source directory and two destination directories\n",
        "src_dir = r'/kaggle/input/d-kap/APPLE_DISEASE_DATASET/LEAF BLOTCH'\n",
        "dst_dir_1 = '/kaggle/working/apple-disease-dataset/datasets/train/cedar_apple_rust'\n",
        "dst_dir_2 = '/kaggle/working/apple-disease-dataset/datasets/test/cedar_apple_rust'\n",
        "\n",
        "# Create the destination directories if they don't already exist\n",
        "if not os.path.exists(dst_dir_1):\n",
        "    os.makedirs(dst_dir_1)\n",
        "if not os.path.exists(dst_dir_2):\n",
        "    os.makedirs(dst_dir_2)\n",
        "\n",
        "# Get a list of all files in the source directory\n",
        "files = os.listdir(src_dir)\n",
        "\n",
        "# Calculate the number of files to copy to each destination directory (60% and 40%)\n",
        "num_files_to_copy_1 = int(len(files) * 0.6)\n",
        "num_files_to_copy_2 = len(files) - num_files_to_copy_1\n",
        "\n",
        "# Randomly select the files to copy to each destination directory\n",
        "files_to_copy_1 = random.sample(files, num_files_to_copy_1)\n",
        "files_to_copy_2 = list(set(files) - set(files_to_copy_1))\n",
        "\n",
        "# Copy the selected files to each destination directory\n",
        "for file_name in files_to_copy_1:\n",
        "    src_file_path = os.path.join(src_dir, file_name)\n",
        "    dst_file_path = os.path.join(dst_dir_1, file_name)\n",
        "    shutil.copy(src_file_path, dst_file_path)\n",
        "\n",
        "for file_name in files_to_copy_2:\n",
        "    src_file_path = os.path.join(src_dir, file_name)\n",
        "    dst_file_path = os.path.join(dst_dir_2, file_name)\n",
        "    shutil.copy(src_file_path, dst_file_path)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-04T07:35:23.362295Z",
          "iopub.execute_input": "2023-05-04T07:35:23.363031Z",
          "iopub.status.idle": "2023-05-04T07:35:24.465658Z",
          "shell.execute_reply.started": "2023-05-04T07:35:23.362979Z",
          "shell.execute_reply": "2023-05-04T07:35:24.461677Z"
        },
        "trusted": true,
        "id": "EKBcU_8UfQDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "# Define paths to source directory and two destination directories\n",
        "src_dir = r'/kaggle/input/d-kap/APPLE_DISEASE_DATASET/SCAB LEAVES'\n",
        "dst_dir_1 = '/kaggle/working/apple-disease-dataset/datasets/train/apple_scab'\n",
        "dst_dir_2 = '/kaggle/working/apple-disease-dataset/datasets/test/apple_scab'\n",
        "\n",
        "# Create the destination directories if they don't already exist\n",
        "if not os.path.exists(dst_dir_1):\n",
        "    os.makedirs(dst_dir_1)\n",
        "if not os.path.exists(dst_dir_2):\n",
        "    os.makedirs(dst_dir_2)\n",
        "\n",
        "# Get a list of all files in the source directory\n",
        "files = os.listdir(src_dir)\n",
        "\n",
        "# Calculate the number of files to copy to each destination directory (60% and 40%)\n",
        "num_files_to_copy_1 = int(len(files) * 0.6)\n",
        "num_files_to_copy_2 = len(files) - num_files_to_copy_1\n",
        "\n",
        "# Randomly select the files to copy to each destination directory\n",
        "files_to_copy_1 = random.sample(files, num_files_to_copy_1)\n",
        "files_to_copy_2 = list(set(files) - set(files_to_copy_1))\n",
        "\n",
        "# Copy the selected files to each destination directory\n",
        "for file_name in files_to_copy_1:\n",
        "    src_file_path = os.path.join(src_dir, file_name)\n",
        "    dst_file_path = os.path.join(dst_dir_1, file_name)\n",
        "    shutil.copy(src_file_path, dst_file_path)\n",
        "\n",
        "for file_name in files_to_copy_2:\n",
        "    src_file_path = os.path.join(src_dir, file_name)\n",
        "    dst_file_path = os.path.join(dst_dir_2, file_name)\n",
        "    shutil.copy(src_file_path, dst_file_path)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-04T07:35:24.47031Z",
          "iopub.execute_input": "2023-05-04T07:35:24.470696Z",
          "iopub.status.idle": "2023-05-04T07:35:25.917879Z",
          "shell.execute_reply.started": "2023-05-04T07:35:24.470661Z",
          "shell.execute_reply": "2023-05-04T07:35:25.916722Z"
        },
        "trusted": true,
        "id": "75-TeHNFfQDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest')\n",
        "\n",
        "# Load the dataset\n",
        "train_data = datagen.flow_from_directory('/kaggle/working/apple-disease-dataset/datasets/train', target_size=(224,224), batch_size=32, class_mode='categorical')\n",
        "val_data = datagen.flow_from_directory('/kaggle/working/apple-disease-dataset/datasets/test', target_size=(224,224), batch_size=32, class_mode='categorical')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-03T08:14:58.131217Z",
          "iopub.execute_input": "2023-05-03T08:14:58.131582Z",
          "iopub.status.idle": "2023-05-03T08:15:05.80337Z",
          "shell.execute_reply.started": "2023-05-03T08:14:58.131546Z",
          "shell.execute_reply": "2023-05-03T08:15:05.802314Z"
        },
        "trusted": true,
        "id": "RL_7PxgXfQDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    validation_split=0.2) # set validation split to 0.2\n",
        "\n",
        "train_data = datagen.flow_from_directory('/kaggle/working/apple-disease-dataset/datasets/train',\n",
        "                                         target_size=(224,224),\n",
        "                                         batch_size=32,\n",
        "                                         class_mode='categorical',\n",
        "                                         subset='training') # set subset to 'training' for train data\n",
        "\n",
        "val_data = datagen.flow_from_directory('/kaggle/working/apple-disease-dataset/datasets/train',\n",
        "                                       target_size=(224,224),\n",
        "                                       batch_size=32,\n",
        "                                       class_mode='categorical',\n",
        "                                       subset='validation') # set subset to 'validation' for validation data\n",
        "\n",
        "test_data = datagen.flow_from_directory('/kaggle/working/apple-disease-dataset/datasets/test',\n",
        "                                         target_size=(224,224),\n",
        "                                         batch_size=32,\n",
        "                                         class_mode='categorical')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-04T07:35:25.920003Z",
          "iopub.execute_input": "2023-05-04T07:35:25.920643Z",
          "iopub.status.idle": "2023-05-04T07:35:33.901822Z",
          "shell.execute_reply.started": "2023-05-04T07:35:25.920605Z",
          "shell.execute_reply": "2023-05-04T07:35:33.9007Z"
        },
        "trusted": true,
        "id": "REIVOf9MfQDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get x_train and y_train\n",
        "x_train, y_train = next(train_data)\n",
        "\n",
        "x_val, y_val = next(val_data)\n",
        "# Get x_test and y_test\n",
        "x_test, y_test = next(test_data)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-04T07:35:33.905282Z",
          "iopub.execute_input": "2023-05-04T07:35:33.905894Z",
          "iopub.status.idle": "2023-05-04T07:35:34.970403Z",
          "shell.execute_reply.started": "2023-05-04T07:35:33.905864Z",
          "shell.execute_reply": "2023-05-04T07:35:34.969365Z"
        },
        "trusted": true,
        "id": "CG-jyCRbfQDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model_1=tf.keras.models.load_model('/kaggle/input/models/best_model_RESNET50_actual.h5')\n",
        "model_2=tf.keras.models.load_model('/kaggle/input/models/final_model_VGG19.h5')\n",
        "model_3=tf.keras.models.load_model('/kaggle/input/models/final_model_densenet201.h5')\n",
        "model_4=tf.keras.models.load_model('/kaggle/input/models/final_model_efficientnet.h5')\n",
        "model_5=tf.keras.models.load_model('/kaggle/input/models/final_model_mobilenetv2.h5')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-04T07:35:34.972159Z",
          "iopub.execute_input": "2023-05-04T07:35:34.972548Z",
          "iopub.status.idle": "2023-05-04T07:36:04.418987Z",
          "shell.execute_reply.started": "2023-05-04T07:35:34.972508Z",
          "shell.execute_reply": "2023-05-04T07:36:04.417836Z"
        },
        "trusted": true,
        "id": "uPYXgKqnfQDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss, train_acc = model_1.evaluate(train_data)\n",
        "val_loss, val_acc = model_1.evaluate(val_data)\n",
        "\n",
        "print(f\"Final training accuracy: {train_acc:.4f}\")\n",
        "print(f\"Final validation accuracy: {val_acc:.4f}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-02T12:08:41.095827Z",
          "iopub.execute_input": "2023-05-02T12:08:41.096221Z",
          "iopub.status.idle": "2023-05-02T12:11:45.829687Z",
          "shell.execute_reply.started": "2023-05-02T12:08:41.096185Z",
          "shell.execute_reply": "2023-05-02T12:11:45.828402Z"
        },
        "trusted": true,
        "id": "P5Yul7MNfQDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-02T11:57:46.334193Z",
          "iopub.execute_input": "2023-05-02T11:57:46.334571Z",
          "iopub.status.idle": "2023-05-02T11:57:46.343914Z",
          "shell.execute_reply.started": "2023-05-02T11:57:46.334539Z",
          "shell.execute_reply": "2023-05-02T11:57:46.342743Z"
        },
        "trusted": true,
        "id": "G_W2QPrvfQDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations\n",
        "\n",
        "# create a list of all possible combinations of base models with length greater than equal 2\n",
        "base_models = [model_1, model_2, model_3, model_4, model_5]\n",
        "base_model_combinations = []\n",
        "for i in range(2, len(base_models) + 1):\n",
        "    base_model_combinations += combinations(base_models, i)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-04T07:36:11.144542Z",
          "iopub.execute_input": "2023-05-04T07:36:11.145448Z",
          "iopub.status.idle": "2023-05-04T07:36:11.154587Z",
          "shell.execute_reply.started": "2023-05-04T07:36:11.145375Z",
          "shell.execute_reply": "2023-05-04T07:36:11.15079Z"
        },
        "trusted": true,
        "id": "YKwdyEmYfQDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-04T07:36:13.529132Z",
          "iopub.execute_input": "2023-05-04T07:36:13.529862Z",
          "iopub.status.idle": "2023-05-04T07:36:13.537244Z",
          "shell.execute_reply.started": "2023-05-04T07:36:13.529824Z",
          "shell.execute_reply": "2023-05-04T07:36:13.536173Z"
        },
        "trusted": true,
        "id": "4b6kZhFMfQDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# create a dictionary to store the predictions of base models on the validation set\n",
        "base_model_predictions = {}\n",
        "\n",
        "# loop over all base model combinations\n",
        "for combination in base_model_combinations:\n",
        "    # create a list to store the predictions of the current base model combination\n",
        "    combination_predictions = []\n",
        "\n",
        "    # loop over all base models in the current combination\n",
        "    for base_model in combination:\n",
        "        # get the predictions of the current base model on the validation set\n",
        "        predictions = base_model.predict(x_train)\n",
        "        combination_predictions.append(predictions)\n",
        "\n",
        "    # concatenate the predictions of all base models in the current combination\n",
        "    combination_predictions = np.concatenate(combination_predictions, axis=1)\n",
        "\n",
        "    # store the predictions of the current base model combination in the dictionary\n",
        "    base_model_predictions[combination] = combination_predictions\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-04T07:40:33.18604Z",
          "iopub.execute_input": "2023-05-04T07:40:33.186767Z",
          "iopub.status.idle": "2023-05-04T07:40:44.865889Z",
          "shell.execute_reply.started": "2023-05-04T07:40:33.186727Z",
          "shell.execute_reply": "2023-05-04T07:40:44.864847Z"
        },
        "trusted": true,
        "id": "-u4SRtAXfQDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(base_model_predictions.values())[0].shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-04T07:51:36.824006Z",
          "iopub.execute_input": "2023-05-04T07:51:36.824515Z",
          "iopub.status.idle": "2023-05-04T07:51:36.83293Z",
          "shell.execute_reply.started": "2023-05-04T07:51:36.824469Z",
          "shell.execute_reply": "2023-05-04T07:51:36.831842Z"
        },
        "trusted": true,
        "id": "z_0komvmfQDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_val.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-04T07:53:31.444291Z",
          "iopub.execute_input": "2023-05-04T07:53:31.445205Z",
          "iopub.status.idle": "2023-05-04T07:53:31.4541Z",
          "shell.execute_reply.started": "2023-05-04T07:53:31.445157Z",
          "shell.execute_reply": "2023-05-04T07:53:31.452907Z"
        },
        "trusted": true,
        "id": "lnjYVvXVfQDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.tile(x_val,3).shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-04T07:53:10.311717Z",
          "iopub.execute_input": "2023-05-04T07:53:10.312324Z",
          "iopub.status.idle": "2023-05-04T07:53:10.335846Z",
          "shell.execute_reply.started": "2023-05-04T07:53:10.312286Z",
          "shell.execute_reply": "2023-05-04T07:53:10.334848Z"
        },
        "trusted": true,
        "id": "0OzS4tplfQDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "combination_accuracies = []\n",
        "# loop over all base model combinations\n",
        "for combination, predictions in base_model_predictions.items():\n",
        "    # define the meta model architecture\n",
        "    meta_model = Sequential([Dense(152, activation='relu', input_shape=(len(combination)*4,)),\n",
        "                             Dropout(0.5),\n",
        "                             Dense(64, activation='relu'),\n",
        "                             Dropout(0.3),\n",
        "                         Dense(4, activation='softmax')])\n",
        "    meta_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # train the meta model on the predictions of the current base model combination\n",
        "    meta_model.fit(predictions, y_train, epochs=10, batch_size=32, verbose=0)\n",
        "\n",
        "    # get the predictions of the current base model combination on the test set\n",
        "    test_predictions = []\n",
        "\n",
        "    train_predictions=[]\n",
        "    for base_model in combination:\n",
        "        predictions_train = base_model.predict(x_train)\n",
        "        train_predictions.append(predictions_train)\n",
        "    train_predictions = np.concatenate(train_predictions, axis=1)\n",
        "\n",
        "    for base_model in combination:\n",
        "        predictions_test = base_model.predict(x_test)\n",
        "        test_predictions.append(predictions_test)\n",
        "    test_predictions = np.concatenate(test_predictions, axis=1)\n",
        "\n",
        "    # evaluate the meta model on the test set\n",
        "    _, test_accuracy = meta_model.evaluate(test_predictions, y_test, verbose=0)\n",
        "    _, train_accuracy = meta_model.evaluate(train_predictions, y_train, verbose=0)\n",
        "    # print the accuracy of the current base model combination\n",
        "    print(f'Accuracy for {len(combination)} base models_train: {train_accuracy},base_model_test:{test_accuracy}')\n",
        "    #print(f'Accuracy for {len(combination)} ,base_model_test:{test_accuracy}')\n",
        "\n",
        "    # add the accuracy of the current base model combination to the list\n",
        "    combination_accuracies.append((combination,train_accuracy,test_accuracy))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-04T07:55:30.86076Z",
          "iopub.execute_input": "2023-05-04T07:55:30.861821Z",
          "iopub.status.idle": "2023-05-04T07:56:33.968338Z",
          "shell.execute_reply.started": "2023-05-04T07:55:30.861782Z",
          "shell.execute_reply": "2023-05-04T07:56:33.967285Z"
        },
        "trusted": true,
        "id": "8IFiTfoSfQDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(combination_accuracies,key=lambda x: (x[2], x[1],5-len(x[0])),reverse=True)[0:10]\n",
        "#[1,4,5]\n",
        "#[2,3,4]\n",
        "#[2,3,5]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-04T07:56:50.773962Z",
          "iopub.execute_input": "2023-05-04T07:56:50.774548Z",
          "iopub.status.idle": "2023-05-04T07:56:50.790948Z",
          "shell.execute_reply.started": "2023-05-04T07:56:50.774503Z",
          "shell.execute_reply": "2023-05-04T07:56:50.789737Z"
        },
        "trusted": true,
        "id": "t15Bj4FUfQDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loop through all layers in the model\n",
        "for layer in model_1.layers:\n",
        "    # set the new name for the layer\n",
        "    layer._name = 'model_1_' + layer.name\n",
        "for layer in model_2.layers:\n",
        "    # set the new name for the layer\n",
        "    layer._name = 'model_2_' + layer.name\n",
        "for layer in model_3.layers:\n",
        "    # set the new name for the layer\n",
        "    layer._name = 'model_3_' + layer.name\n",
        "for layer in model_4.layers:\n",
        "    # set the new name for the layer\n",
        "    layer._name = 'model_4_' + layer.name\n",
        "for layer in model_5.layers:\n",
        "    # set the new name for the layer\n",
        "    layer._name = 'model_5_' + layer.name\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-04T08:01:16.508599Z",
          "iopub.execute_input": "2023-05-04T08:01:16.509262Z",
          "iopub.status.idle": "2023-05-04T08:01:16.517107Z",
          "shell.execute_reply.started": "2023-05-04T08:01:16.509223Z",
          "shell.execute_reply": "2023-05-04T08:01:16.516046Z"
        },
        "trusted": true,
        "id": "deQMR4T4fQDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#[1,4,5]\n",
        "from keras.regularizers import l2\n",
        "from keras.models import Model\n",
        "from keras.layers import concatenate, Dense, Dropout\n",
        "\n",
        "# assume you have three models named model1, model2, model3\n",
        "# and a meta-model named dense_meta_model\n",
        "model_1.trainable = False\n",
        "model_4.trainable=False\n",
        "model_5.trainable = False\n",
        "# get the output layers of individual models\n",
        "output1 = model_1.output\n",
        "output2 = model_4.output\n",
        "output3 = model_5.output\n",
        "\n",
        "# concatenate the outputs of the individual models\n",
        "merged = concatenate([output1, output2, output3])\n",
        "meta_model_1 = Dense(152, activation='relu',kernel_regularizer=l2(0.01), name='meta_model_1')(merged)\n",
        "meta_model_2 = Dropout(0.5, name='meta_model_2')(meta_model_1)\n",
        "meta_model_3 = Dense(64, activation='relu',kernel_regularizer=l2(0.01), name='meta_model_3')(meta_model_2)\n",
        "meta_model_4 = Dropout(0.3, name='meta_model_4')(meta_model_3)\n",
        "final_output = Dense(4, activation='softmax', name='final_output')(meta_model_4)\n",
        "\n",
        "# pass the concatenated output to the meta-model\n",
        "#final_output = meta_model(merged)\n",
        "\n",
        "# create the final model with inputs from the individual models\n",
        "# and output from the meta-model\n",
        "final_model_1 = Model(inputs=[model_1.input,model_4.input,model_5.input], outputs=final_output)\n",
        "\n",
        "# compile the final model\n",
        "final_model_1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-04T08:19:07.331721Z",
          "iopub.execute_input": "2023-05-04T08:19:07.333023Z",
          "iopub.status.idle": "2023-05-04T08:19:07.428728Z",
          "shell.execute_reply.started": "2023-05-04T08:19:07.332967Z",
          "shell.execute_reply": "2023-05-04T08:19:07.427672Z"
        },
        "trusted": true,
        "id": "ioX_G57-fQDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#[2,3,4]\n",
        "from keras.models import Model\n",
        "from keras.layers import concatenate, Dense, Dropout\n",
        "\n",
        "# assume you have three models named model1, model2, model3\n",
        "# and a meta-model named dense_meta_model\n",
        "model_2.trainable = False\n",
        "model_3.trainable=False\n",
        "model_4.trainable = False\n",
        "# get the output layers of individual models\n",
        "output1 = model_2.output\n",
        "output2 = model_3.output\n",
        "output3 = model_4.output\n",
        "\n",
        "# concatenate the outputs of the individual models\n",
        "merged = concatenate([output1, output2, output3])\n",
        "meta_model_1 = Dense(152, activation='relu',kernel_regularizer=l2(0.01), name='meta_model_1')(merged)\n",
        "meta_model_2 = Dropout(0.5, name='meta_model_2')(meta_model_1)\n",
        "meta_model_3 = Dense(64, activation='relu',kernel_regularizer=l2(0.01), name='meta_model_3')(meta_model_2)\n",
        "meta_model_4 = Dropout(0.3, name='meta_model_4')(meta_model_3)\n",
        "final_output = Dense(4, activation='softmax', name='final_output')(meta_model_4)\n",
        "\n",
        "# pass the concatenated output to the meta-model\n",
        "#final_output = meta_model(merged)\n",
        "\n",
        "# create the final model with inputs from the individual models\n",
        "# and output from the meta-model\n",
        "final_model_2 = Model(inputs=[model_2.input,model_3.input,model_4.input], outputs=final_output)\n",
        "\n",
        "# compile the final model\n",
        "final_model_2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-04T08:19:09.213656Z",
          "iopub.execute_input": "2023-05-04T08:19:09.214716Z",
          "iopub.status.idle": "2023-05-04T08:19:09.325803Z",
          "shell.execute_reply.started": "2023-05-04T08:19:09.214663Z",
          "shell.execute_reply": "2023-05-04T08:19:09.324779Z"
        },
        "trusted": true,
        "id": "KtezexIZfQDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#[2,3,5]\n",
        "from keras.models import Model\n",
        "from keras.layers import concatenate, Dense, Dropout\n",
        "\n",
        "# assume you have three models named model1, model2, model3\n",
        "# and a meta-model named dense_meta_model\n",
        "model_2.trainable = False\n",
        "model_3.trainable=False\n",
        "model_5.trainable = False\n",
        "# get the output layers of individual models\n",
        "output1 = model_2.output\n",
        "output2 = model_3.output\n",
        "output3 = model_5.output\n",
        "\n",
        "# concatenate the outputs of the individual models\n",
        "merged = concatenate([output1, output2, output3])\n",
        "meta_model_1 = Dense(152, activation='relu',kernel_regularizer=l2(0.01), name='meta_model_1')(merged)\n",
        "meta_model_2 = Dropout(0.5, name='meta_model_2')(meta_model_1)\n",
        "meta_model_3 = Dense(64, activation='relu',kernel_regularizer=l2(0.01), name='meta_model_3')(meta_model_2)\n",
        "meta_model_4 = Dropout(0.3, name='meta_model_4')(meta_model_3)\n",
        "final_output = Dense(4, activation='softmax', name='final_output')(meta_model_4)\n",
        "\n",
        "# pass the concatenated output to the meta-model\n",
        "#final_output = meta_model(merged)\n",
        "\n",
        "# create the final model with inputs from the individual models\n",
        "# and output from the meta-model\n",
        "final_model_3 = Model(inputs=[model_2.input,model_3.input,model_5.input], outputs=final_output)\n",
        "\n",
        "# compile the final model\n",
        "final_model_3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-04T08:19:15.7209Z",
          "iopub.execute_input": "2023-05-04T08:19:15.721499Z",
          "iopub.status.idle": "2023-05-04T08:19:15.820042Z",
          "shell.execute_reply.started": "2023-05-04T08:19:15.72146Z",
          "shell.execute_reply": "2023-05-04T08:19:15.819056Z"
        },
        "trusted": true,
        "id": "Xf0Nw7sUfQDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the input data for prediction\n",
        "X_test = [x_test, x_test,x_test]\n",
        "X_train=[x_train,x_train,x_train]\n",
        "X_val=[x_val,x_val,x_val]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-04T08:07:03.954951Z",
          "iopub.execute_input": "2023-05-04T08:07:03.955362Z",
          "iopub.status.idle": "2023-05-04T08:07:03.961263Z",
          "shell.execute_reply.started": "2023-05-04T08:07:03.955326Z",
          "shell.execute_reply": "2023-05-04T08:07:03.960104Z"
        },
        "trusted": true,
        "id": "wsjnxtGPfQDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_model_1.fit(X_train, y_train, epochs=20,validation_data=(X_val,y_val))\n",
        "final_model_2.fit(X_train, y_train, epochs=20,validation_data=(X_val,y_val))\n",
        "final_model_3.fit(X_train, y_train, epochs=20,validation_data=(X_val,y_val))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-04T08:19:37.389991Z",
          "iopub.execute_input": "2023-05-04T08:19:37.390623Z",
          "iopub.status.idle": "2023-05-04T08:21:21.914256Z",
          "shell.execute_reply.started": "2023-05-04T08:19:37.390583Z",
          "shell.execute_reply": "2023-05-04T08:21:21.913086Z"
        },
        "trusted": true,
        "id": "G1lyEEe-fQDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_1, train_acc_1 = final_model_1.evaluate(X_train,y_train)\n",
        "val_loss_1, test_acc_1 = final_model_1.evaluate(X_test,y_test)\n",
        "train_loss_1, val_acc_1 = final_model_1.evaluate(X_val,y_val)\n",
        "\n",
        "print(f\"Final training accuracy: {train_acc_1:.4f},Final validation accuracy: {val_acc_1:.4f},Final test accuracy: {test_acc_1:.4f}\")\n",
        "train_loss_2, train_acc_2 = final_model_2.evaluate(X_train,y_train)\n",
        "val_loss_2, test_acc_2 = final_model_2.evaluate(X_test,y_test)\n",
        "train_loss_2, val_acc_2 = final_model_2.evaluate(X_val,y_val)\n",
        "\n",
        "print(f\"Final training accuracy: {train_acc_2:.4f},Final validation accuracy: {val_acc_2:.4f},Final test accuracy: {test_acc_2:.4f}\")\n",
        "train_loss_3, train_acc_3 = final_model_3.evaluate(X_train,y_train)\n",
        "val_loss_3, test_acc_3 = final_model_3.evaluate(X_test,y_test)\n",
        "train_loss_3, val_acc_3 = final_model_3.evaluate(X_val,y_val)\n",
        "\n",
        "print(f\"Final training accuracy: {train_acc_1:.4f},Final validation accuracy: {val_acc_1:.4f},Final test accuracy: {test_acc_1:.4f}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-04T08:21:21.916091Z",
          "iopub.execute_input": "2023-05-04T08:21:21.917183Z",
          "iopub.status.idle": "2023-05-04T08:21:25.363851Z",
          "shell.execute_reply.started": "2023-05-04T08:21:21.917138Z",
          "shell.execute_reply": "2023-05-04T08:21:25.362867Z"
        },
        "trusted": true,
        "id": "B6tPnuztfQDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import numpy as np\n",
        "\n",
        "# convert one-hot encoded labels to integers\n",
        "y_train_classes = np.argmax(y_train, axis=1)\n",
        "y_val_classes = np.argmax(y_val, axis=1)\n",
        "\n",
        "combination_accuracies = []\n",
        "\n",
        "# loop over all base model combinations\n",
        "for combination, predictions in base_model_predictions.items():\n",
        "    # concatenate base model predictions\n",
        "    train_predictions = np.concatenate([base_model.predict(x_train) for base_model in combination], axis=1)\n",
        "    val_predictions = np.concatenate([base_model.predict(x_val) for base_model in combination], axis=1)\n",
        "\n",
        "    # define the SVM meta-model\n",
        "    svm = SVC(probability=True)\n",
        "    # define hyperparameters to search over\n",
        "    param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
        "    # perform grid search over hyperparameters\n",
        "    svm_grid = GridSearchCV(svm, param_grid=param_grid, cv=5, n_jobs=-1)\n",
        "    svm_grid.fit(train_predictions, y_train_classes)\n",
        "\n",
        "    # get the predictions of the current base model combination on the validation set\n",
        "    val_predictions = np.concatenate([base_model.predict(x_val) for base_model in combination], axis=1)\n",
        "\n",
        "    # evaluate the SVM meta-model on the validation set\n",
        "    val_accuracy = svm_grid.score(val_predictions, y_val_classes)\n",
        "    train_accuracy = svm_grid.score(train_predictions, y_train_classes)\n",
        "\n",
        "    # print the accuracy of the current base model combination\n",
        "    print(f'Accuracy for {len(combination)} base models: train: {train_accuracy}, val: {val_accuracy}')\n",
        "\n",
        "    # add the accuracy of the current base model combination to the list\n",
        "    combination_accuracies.append((combination, train_accuracy, val_accuracy))\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-04T04:08:32.765301Z",
          "iopub.execute_input": "2023-05-04T04:08:32.765993Z",
          "iopub.status.idle": "2023-05-04T04:09:14.260887Z",
          "shell.execute_reply.started": "2023-05-04T04:08:32.765954Z",
          "shell.execute_reply": "2023-05-04T04:09:14.259239Z"
        },
        "trusted": true,
        "id": "7lv8nidTfQDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(combination_accuracies,key=lambda x: (x[2], x[1],5-len(x[0])),reverse=True)[0:5]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-04T04:10:39.671403Z",
          "iopub.execute_input": "2023-05-04T04:10:39.671846Z",
          "iopub.status.idle": "2023-05-04T04:10:39.682267Z",
          "shell.execute_reply.started": "2023-05-04T04:10:39.671808Z",
          "shell.execute_reply": "2023-05-04T04:10:39.6812Z"
        },
        "trusted": true,
        "id": "PVLbw2RDfQDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#[model_2,model_3]\n",
        "#[model_2,model_4]\n",
        "#[model_3,model_4]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-04T04:12:19.255326Z",
          "iopub.execute_input": "2023-05-04T04:12:19.256278Z",
          "iopub.status.idle": "2023-05-04T04:12:19.263733Z",
          "shell.execute_reply.started": "2023-05-04T04:12:19.256206Z",
          "shell.execute_reply": "2023-05-04T04:12:19.262438Z"
        },
        "trusted": true,
        "id": "so7ZyckSfQDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r-bAlrNrfQDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import concatenate, Dense, Dropout\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# assume you have three models named model1, model2, model3\n",
        "# and a meta-model named svm_meta_model\n",
        "model_2.trainable = False\n",
        "model_3.trainable = False\n",
        "\n",
        "# get predictions of model_2 and model_3 on the validation set\n",
        "predictions_2_val = model_2.predict(x_val)\n",
        "predictions_3_val = model_3.predict(x_val)\n",
        "\n",
        "# concatenate the predictions into a single feature matrix\n",
        "base_model_predictions = np.concatenate((predictions_2_val, predictions_3_val), axis=1)\n",
        "\n",
        "# define the parameter grid for hyperparameter tuning\n",
        "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [0.1, 1, 10, 100]}\n",
        "\n",
        "# create the SVM meta model\n",
        "svm_meta_model = SVC(kernel='rbf', probability=True)\n",
        "y_val=[np.argmax(x) for x in y_val]\n",
        "# perform hyperparameter tuning using grid search with cross-validation\n",
        "svm_meta_model_tuned = GridSearchCV(svm_meta_model, param_grid, cv=5, scoring='accuracy')\n",
        "svm_meta_model_tuned.fit(base_model_predictions, y_val)\n",
        "\n",
        "# get the best hyperparameters\n",
        "best_C = svm_meta_model_tuned.best_params_['C']\n",
        "best_gamma = svm_meta_model_tuned.best_params_['gamma']\n",
        "\n",
        "# create the final SVM meta model using the best hyperparameters\n",
        "svm_meta_model_final = SVC(kernel='rbf', probability=True, C=best_C, gamma=best_gamma)\n",
        "\n",
        "# get predictions of model_2 and model_3 on the test set\n",
        "predictions_2_test = model_2.predict(x_test)\n",
        "predictions_3_test = model_3.predict(x_test)\n",
        "\n",
        "# concatenate the predictions into a single feature matrix\n",
        "base_model_predictions_test = np.concatenate((predictions_2_test, predictions_3_test), axis=1)\n",
        "\n",
        "# train the final SVM meta model on the concatenated base model predictions\n",
        "svm_meta_model_final.fit(base_model_predictions, y_val)\n",
        "y_test=[np.argmax(x) for x in y_test]\n",
        "# evaluate the final SVM meta model on the test set\n",
        "svm_meta_model_accuracy = svm_meta_model_final.score(base_model_predictions_test, y_test)\n",
        "\n",
        "print('Final SVM Meta Model Accuracy:', svm_meta_model_accuracy)\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-04T04:22:58.698518Z",
          "iopub.execute_input": "2023-05-04T04:22:58.698986Z",
          "iopub.status.idle": "2023-05-04T04:22:59.621883Z",
          "shell.execute_reply.started": "2023-05-04T04:22:58.698943Z",
          "shell.execute_reply": "2023-05-04T04:22:59.620663Z"
        },
        "trusted": true,
        "id": "Wwj1urzEfQDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import concatenate, Dense, Dropout\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# assume you have three models named model1, model2, model3\n",
        "# and a meta-model named svm_meta_model\n",
        "model_2.trainable = False\n",
        "model_4.trainable = False\n",
        "\n",
        "# get predictions of model_2 and model_3 on the validation set\n",
        "predictions_2_val = model_2.predict(x_val)\n",
        "predictions_3_val = model_4.predict(x_val)\n",
        "\n",
        "# concatenate the predictions into a single feature matrix\n",
        "base_model_predictions = np.concatenate((predictions_2_val, predictions_3_val), axis=1)\n",
        "\n",
        "# define the parameter grid for hyperparameter tuning\n",
        "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [0.1, 1, 10, 100]}\n",
        "\n",
        "# create the SVM meta model\n",
        "svm_meta_model = SVC(kernel='rbf', probability=True)\n",
        "#y_val=[np.argmax(x) for x in y_val]\n",
        "# perform hyperparameter tuning using grid search with cross-validation\n",
        "svm_meta_model_tuned = GridSearchCV(svm_meta_model, param_grid, cv=5, scoring='accuracy')\n",
        "svm_meta_model_tuned.fit(base_model_predictions, y_val)\n",
        "\n",
        "# get the best hyperparameters\n",
        "best_C = svm_meta_model_tuned.best_params_['C']\n",
        "best_gamma = svm_meta_model_tuned.best_params_['gamma']\n",
        "\n",
        "# create the final SVM meta model using the best hyperparameters\n",
        "svm_meta_model_final = SVC(kernel='rbf', probability=True, C=best_C, gamma=best_gamma)\n",
        "\n",
        "# get predictions of model_2 and model_3 on the test set\n",
        "predictions_2_test = model_2.predict(x_test)\n",
        "predictions_3_test = model_4.predict(x_test)\n",
        "\n",
        "# concatenate the predictions into a single feature matrix\n",
        "base_model_predictions_test = np.concatenate((predictions_2_test, predictions_3_test), axis=1)\n",
        "\n",
        "# train the final SVM meta model on the concatenated base model predictions\n",
        "svm_meta_model_final.fit(base_model_predictions, y_val)\n",
        "#y_test=[np.argmax(x) for x in y_test]\n",
        "# evaluate the final SVM meta model on the test set\n",
        "svm_meta_model_accuracy = svm_meta_model_final.score(base_model_predictions_test, y_test)\n",
        "\n",
        "print('Final SVM Meta Model Accuracy:', svm_meta_model_accuracy)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-04T04:24:10.155647Z",
          "iopub.execute_input": "2023-05-04T04:24:10.156479Z",
          "iopub.status.idle": "2023-05-04T04:24:11.125707Z",
          "shell.execute_reply.started": "2023-05-04T04:24:10.156438Z",
          "shell.execute_reply": "2023-05-04T04:24:11.122034Z"
        },
        "trusted": true,
        "id": "GOk2b5KJfQDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import concatenate, Dense, Dropout\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# assume you have three models named model1, model2, model3\n",
        "# and a meta-model named svm_meta_model\n",
        "model_3.trainable = False\n",
        "model_4.trainable = False\n",
        "\n",
        "# get predictions of model_2 and model_3 on the validation set\n",
        "predictions_2_val = model_3.predict(x_val)\n",
        "predictions_3_val = model_4.predict(x_val)\n",
        "\n",
        "# concatenate the predictions into a single feature matrix\n",
        "base_model_predictions = np.concatenate((predictions_2_val, predictions_3_val), axis=1)\n",
        "\n",
        "# define the parameter grid for hyperparameter tuning\n",
        "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [0.1, 1, 10, 100]}\n",
        "\n",
        "# create the SVM meta model\n",
        "svm_meta_model = SVC(kernel='rbf', probability=True)\n",
        "#y_val=[np.argmax(x) for x in y_val]\n",
        "# perform hyperparameter tuning using grid search with cross-validation\n",
        "svm_meta_model_tuned = GridSearchCV(svm_meta_model, param_grid, cv=5, scoring='accuracy')\n",
        "svm_meta_model_tuned.fit(base_model_predictions, y_val)\n",
        "\n",
        "# get the best hyperparameters\n",
        "best_C = svm_meta_model_tuned.best_params_['C']\n",
        "best_gamma = svm_meta_model_tuned.best_params_['gamma']\n",
        "\n",
        "# create the final SVM meta model using the best hyperparameters\n",
        "svm_meta_model_final = SVC(kernel='rbf', probability=True, C=best_C, gamma=best_gamma)\n",
        "\n",
        "# get predictions of model_2 and model_3 on the test set\n",
        "predictions_2_test = model_3.predict(x_test)\n",
        "predictions_3_test = model_4.predict(x_test)\n",
        "predictions_2_train = model_3.predict(x_train)\n",
        "predictions_3_train = model_4.predict(x_train)\n",
        "predictions_2_val = model_3.predict(x_val)\n",
        "predictions_3_val = model_4.predict(x_val)\n",
        "# concatenate the predictions into a single feature matrix\n",
        "base_model_predictions_test = np.concatenate((predictions_2_test, predictions_3_test), axis=1)\n",
        "base_model_predictions_train = np.concatenate((predictions_2_train, predictions_3_train), axis=1)\n",
        "base_model_predictions_val = np.concatenate((predictions_2_val, predictions_3_val), axis=1)\n",
        "\n",
        "# train the final SVM meta model on the concatenated base model predictions\n",
        "svm_meta_model_final.fit(base_model_predictions, y_val)\n",
        "\n",
        "y_train=[np.argmax(x) for x in y_train]\n",
        "# evaluate the final SVM meta model on the test set\n",
        "svm_meta_model_accuracy = svm_meta_model_final.score(base_model_predictions_test, y_test)\n",
        "svm_meta_model_accuracy_train = svm_meta_model_final.score(base_model_predictions_train, y_train)\n",
        "svm_meta_model_accuracy_val = svm_meta_model_final.score(base_model_predictions_val, y_val)\n",
        "\n",
        "print('Final SVM Meta Model Accuracy:', svm_meta_model_accuracy)\n",
        "print('Final SVM Meta Model Accuracy_train:', svm_meta_model_accuracy_train)\n",
        "print('Final SVM Meta Model Accuracy_val:', svm_meta_model_accuracy_val)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-04T04:30:57.725797Z",
          "iopub.execute_input": "2023-05-04T04:30:57.726186Z",
          "iopub.status.idle": "2023-05-04T04:30:59.570931Z",
          "shell.execute_reply.started": "2023-05-04T04:30:57.726152Z",
          "shell.execute_reply": "2023-05-04T04:30:59.569655Z"
        },
        "trusted": true,
        "id": "nE7zYkLGfQDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WOUIkL9TfQDf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}